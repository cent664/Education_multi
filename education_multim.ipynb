{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34cb2cc4-c1fd-49d8-a33c-cb1aa45d1c58",
   "metadata": {},
   "source": [
    "**Prereqs**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100827d-996c-49ea-bf60-5c74cd6350ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed\n",
    "# %pip install torch torchvision pandas opencv-python tqdm\n",
    "# Debian/Ubuntu system packages (in a terminal): sudo apt-get install ffmpeg\n",
    "# conda install -c conda-forge ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c500523-0daf-4561-a8dd-20786372260e",
   "metadata": {},
   "source": [
    "**Config**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488ba69-f26b-449c-8c4c-06026dc4eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Single source video (you can expand to many later)\n",
    "INPUT_VIDEO = Path(\"edu_data/1908 2nd-observation.mov\")\n",
    "\n",
    "# Workspace for clips + metadata + checkpoints\n",
    "WORK_DIR = Path(\"edu_data/1908_2nd_observation_prepped_nb\")\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Clip parameters\n",
    "SEGMENT_SECONDS = 5\n",
    "TARGET_FPS = 15          # re-sample to this fps (approx) when extracting\n",
    "SHORT_SIDE = 224         # resize short side; preserve aspect; make even dims\n",
    "\n",
    "# SSL training params\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 4\n",
    "CLIP_FRAMES = 16         # frames per training sample\n",
    "NUM_WORKERS = 0          # start with 0 in notebooks\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd5cc6-2a03-42a0-bb70-901016c3b5b5",
   "metadata": {},
   "source": [
    "**Extract 5‑second clips without FFmpeg (OpenCV → frame folders)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53770bee-8e10-4b2a-b6d3-841a01151920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, math, os, pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def _resize_keep_short_side(frame, short_side=224):\n",
    "    h, w = frame.shape[:2]\n",
    "    if h <= 0 or w <= 0:\n",
    "        return frame\n",
    "    if h < w:\n",
    "        new_h = short_side\n",
    "        new_w = int(round(w * (short_side / h)))\n",
    "    else:\n",
    "        new_w = short_side\n",
    "        new_h = int(round(h * (short_side / w)))\n",
    "    # make even dims for downstream 3D models\n",
    "    new_h += new_h % 2\n",
    "    new_w += new_w % 2\n",
    "    return cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def extract_clip_folders_opencv(\n",
    "    input_video: Path,\n",
    "    out_dir: Path,\n",
    "    segment_seconds: int = 5,\n",
    "    target_fps: int = 15,\n",
    "    short_side: int = 224,\n",
    "    img_ext: str = \".png\"  # .jpg is smaller but lossy; pick what you prefer\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Decode with OpenCV, re-sample frames to ~target_fps, write each 5s chunk as a folder of frames.\n",
    "    Returns a metadata DataFrame with one row per clip.\n",
    "    \"\"\"\n",
    "    out_clips_dir = out_dir / \"clips_frames\"\n",
    "    out_clips_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(str(input_video))\n",
    "    assert cap.isOpened(), f\"Could not open: {input_video}\"\n",
    "\n",
    "    src_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    duration_s = total_frames / max(src_fps, 1e-6)\n",
    "\n",
    "    # Determine frame sampling stride to approximate TARGET_FPS\n",
    "    stride = max(int(round(src_fps / target_fps)), 1)\n",
    "    eff_fps = src_fps / stride\n",
    "\n",
    "    frames_per_clip = int(round(segment_seconds * eff_fps))\n",
    "    clip_idx = 0\n",
    "    frame_idx = 0\n",
    "    sampled_idx = 0\n",
    "\n",
    "    rows = []\n",
    "    writer_count_in_clip = 0\n",
    "    clip_dir = None\n",
    "    start_time = 0.0\n",
    "\n",
    "    pbar = tqdm(total=total_frames, desc=\"Decoding & writing frames\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            # flush last clip\n",
    "            if writer_count_in_clip > 0 and clip_dir is not None:\n",
    "                end_time = start_time + (writer_count_in_clip / eff_fps)\n",
    "                rows.append(dict(\n",
    "                    clip_id=f\"{clip_idx:06d}\",\n",
    "                    clip_dir=str(clip_dir),\n",
    "                    source_video=str(input_video),\n",
    "                    start_s=round(start_time, 3),\n",
    "                    end_s=round(end_time, 3),\n",
    "                    fps=round(eff_fps, 3),\n",
    "                    num_frames=writer_count_in_clip,\n",
    "                    duration_s=round(end_time - start_time, 3)\n",
    "                ))\n",
    "            break\n",
    "\n",
    "        # keep about TARGET_FPS by skipping frames\n",
    "        if frame_idx % stride == 0:\n",
    "            # resize/pad to even dims on the fly\n",
    "            frame = _resize_keep_short_side(frame, short_side=short_side)\n",
    "\n",
    "            # start a new clip folder if needed\n",
    "            if writer_count_in_clip == 0:\n",
    "                clip_dir = out_clips_dir / f\"{clip_idx:06d}\"\n",
    "                clip_dir.mkdir(parents=True, exist_ok=True)\n",
    "                start_time = (sampled_idx / eff_fps)\n",
    "\n",
    "            # write frame\n",
    "            out_path = clip_dir / f\"{writer_count_in_clip:06d}{img_ext}\"\n",
    "            cv2.imwrite(str(out_path), frame)\n",
    "            writer_count_in_clip += 1\n",
    "            sampled_idx += 1\n",
    "\n",
    "            # close the clip if we reached 5 seconds worth of sampled frames\n",
    "            if writer_count_in_clip >= frames_per_clip:\n",
    "                end_time = start_time + (writer_count_in_clip / eff_fps)\n",
    "                rows.append(dict(\n",
    "                    clip_id=f\"{clip_idx:06d}\",\n",
    "                    clip_dir=str(clip_dir),\n",
    "                    source_video=str(input_video),\n",
    "                    start_s=round(start_time, 3),\n",
    "                    end_s=round(end_time, 3),\n",
    "                    fps=round(eff_fps, 3),\n",
    "                    num_frames=writer_count_in_clip,\n",
    "                    duration_s=round(end_time - start_time, 3)\n",
    "                ))\n",
    "                clip_idx += 1\n",
    "                writer_count_in_clip = 0\n",
    "                clip_dir = None\n",
    "\n",
    "        frame_idx += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = out_dir / \"clips_metadata.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Wrote {len(df)} clips to {out_clips_dir}\")\n",
    "    print(f\"Metadata: {csv_path}\")\n",
    "    return df\n",
    "\n",
    "df_meta = extract_clip_folders_opencv(\n",
    "    INPUT_VIDEO, WORK_DIR,\n",
    "    segment_seconds=SEGMENT_SECONDS, target_fps=TARGET_FPS, short_side=SHORT_SIDE,\n",
    "    img_ext=\".jpg\"  # switch to \".png\" if you prefer lossless\n",
    ")\n",
    "\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ff921-ccb2-4722-8c09-ebc81cb5f07f",
   "metadata": {},
   "source": [
    "**Visual sanity check (first clip)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8bbe27-0b66-47e8-a637-c2d67d91ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "first_clip_dir = Path(df_meta.iloc[0][\"clip_dir\"])\n",
    "first_frames = sorted(glob.glob(str(first_clip_dir / \"*.jpg\")))[:4]\n",
    "display(Image.open(first_frames[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae5a8c-b4ba-46ce-8f9c-d469309133cd",
   "metadata": {},
   "source": [
    "**Dataset & SSL model (SimCLR on 3D ResNet‑18)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b6eda-8a3a-4ae5-96a3-c5a967830590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as T\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FrameFolderDatasetSSL(Dataset):\n",
    "    def __init__(self, metadata_csv, clip_frames=16, resize_hw=224, training=True, img_glob=\"*.jpg\"):\n",
    "        import pandas as pd\n",
    "        self.df = pd.read_csv(metadata_csv)\n",
    "        self.clip_frames = clip_frames\n",
    "        self.training = training\n",
    "        self.img_glob = img_glob\n",
    "        self.tx = T.Compose([\n",
    "            T.ToImage(),  # HWC->CHW for PIL/numpy\n",
    "            T.Resize((resize_hw, resize_hw)),\n",
    "            T.RandomHorizontalFlip(p=0.5) if training else T.Identity(),\n",
    "            T.ColorJitter(0.4, 0.4, 0.4, 0.2) if training else T.Identity(),\n",
    "            T.RandomGrayscale(p=0.2) if training else T.Identity(),\n",
    "            T.ToDtype(torch.float32, scale=True),\n",
    "            T.Normalize(mean=[0.45,0.45,0.45], std=[0.225,0.225,0.225]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _load_clip_frames(self, clip_dir):\n",
    "        frames = sorted(glob.glob(os.path.join(clip_dir, self.img_glob)))\n",
    "        return frames\n",
    "\n",
    "    def _sample_indices(self, total, Treq):\n",
    "        if total <= Treq:\n",
    "            idx = list(range(total))\n",
    "            while len(idx) < Treq:\n",
    "                idx += idx[:(Treq - len(idx))]\n",
    "            return idx[:Treq]\n",
    "        start = random.randint(0, total - Treq)\n",
    "        return list(range(start, start + Treq))\n",
    "\n",
    "    def _two_views(self, pil_list):\n",
    "        # pil_list: list of PIL.Image (length = clip_frames)\n",
    "        v1 = torch.stack([self.tx(img) for img in pil_list], dim=1)  # [C,T,H,W]\n",
    "        v2 = torch.stack([self.tx(img) for img in pil_list], dim=1)\n",
    "        return v1, v2\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        clip_dir = self.df.iloc[i][\"clip_dir\"]\n",
    "        files = self._load_clip_frames(clip_dir)\n",
    "        if len(files) == 0:\n",
    "            # create dummy black clip if decode failed\n",
    "            dummy = Image.fromarray(np.zeros((224,224,3), dtype=np.uint8))\n",
    "            pil_list = [dummy for _ in range(self.clip_frames)]\n",
    "            return self._two_views(pil_list)\n",
    "\n",
    "        idx = self._sample_indices(len(files), self.clip_frames)\n",
    "        pil_list = [Image.open(files[k]).convert(\"RGB\") for k in idx]\n",
    "        return self._two_views(pil_list)\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, in_dim, hid=2048, out_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid), nn.BatchNorm1d(hid), nn.ReLU(True),\n",
    "            nn.Linear(hid, out_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class SimCLR3D(nn.Module):\n",
    "    def __init__(self, proj_dim=256):\n",
    "        super().__init__()\n",
    "        m = torchvision.models.video.r3d_18(weights=None)\n",
    "        feat_dim = m.fc.in_features\n",
    "        m.fc = nn.Identity()\n",
    "        self.backbone = m\n",
    "        self.projector = Projector(feat_dim, 2048, proj_dim)\n",
    "    def forward(self, x):  # x: [B,C,T,H,W]\n",
    "        f = self.backbone(x)\n",
    "        z = F.normalize(self.projector(f), dim=1)\n",
    "        return z\n",
    "\n",
    "def info_nce_loss(z1, z2, temperature=0.2):\n",
    "    B = z1.size(0)\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    sim = (z @ z.t()) / temperature\n",
    "    sim.fill_diagonal_(-9e15)\n",
    "    targets = torch.arange(B, device=z.device)\n",
    "    targets = torch.cat([targets + B, targets], dim=0)\n",
    "    return F.cross_entropy(sim, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813636a5-a6e6-473e-9574-69c85f9569d4",
   "metadata": {},
   "source": [
    "**Train (self‑supervised pretraining)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9c650-6e2c-4eef-ab33-bb572c4765e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, glob, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- dataloader (unchanged) ---\n",
    "ds = FrameFolderDatasetSSL(\n",
    "    metadata_csv=WORK_DIR/\"clips_metadata.csv\",\n",
    "    clip_frames=CLIP_FRAMES, resize_hw=224, training=True, img_glob=\"*.jpg\"\n",
    ")\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                num_workers=NUM_WORKERS, pin_memory=(device==\"cuda\"), drop_last=True)\n",
    "\n",
    "# --- model/opt (unchanged) ---\n",
    "model = SimCLR3D(proj_dim=256).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "# --- AMP (speeds up on GPU) ---\n",
    "use_amp = (device == \"cuda\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "# --- optional scheduler: linear warmup -> cosine decay ---\n",
    "steps_per_epoch = len(dl)\n",
    "total_steps = EPOCHS * steps_per_epoch\n",
    "warmup_steps = max(100, steps_per_epoch // 2)  # ~half an epoch warmup\n",
    "\n",
    "def lr_schedule(step):\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / warmup_steps\n",
    "    # cosine decay to 0.1x of base LR\n",
    "    t = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "    return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * t))\n",
    "\n",
    "# --- checkpointing helpers ---\n",
    "CKPT_DIR = Path(WORK_DIR) / \"checkpoints_ssl\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LATEST = CKPT_DIR / \"latest.pt\"\n",
    "BEST = CKPT_DIR / \"best.pt\"\n",
    "METRICS_JSON = CKPT_DIR / \"metrics.json\"\n",
    "\n",
    "def save_ckpt(path, epoch, step, best_loss, best_epoch):\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": opt.state_dict(),\n",
    "        \"scaler\": scaler.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": step,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"hparams\": {\n",
    "            \"clip_frames\": CLIP_FRAMES,\n",
    "            \"target_fps\": TARGET_FPS,\n",
    "            \"short_side\": SHORT_SIDE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"lr_base\": LR,\n",
    "            \"epochs\": EPOCHS,\n",
    "        },\n",
    "    }, path)\n",
    "\n",
    "def load_ckpt_if_exists():\n",
    "    if LATEST.exists():\n",
    "        ckpt = torch.load(LATEST, map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        opt.load_state_dict(ckpt[\"optimizer\"])\n",
    "        try:\n",
    "            scaler.load_state_dict(ckpt[\"scaler\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "        return (ckpt.get(\"epoch\", 0), ckpt.get(\"global_step\", 0),\n",
    "                ckpt.get(\"best_loss\", float(\"inf\")), ckpt.get(\"best_epoch\", -1))\n",
    "    return 0, 0, float(\"inf\"), -1\n",
    "\n",
    "# --- resume if possible ---\n",
    "start_epoch, global_step, best_running_loss, best_epoch = load_ckpt_if_exists()\n",
    "print(f\"Resuming from epoch {start_epoch}, step {global_step} (best_loss={best_running_loss:.4f} @ epoch {best_epoch})\")\n",
    "\n",
    "# --- training loop ---\n",
    "log_every = 50                      # steps\n",
    "save_every_steps = steps_per_epoch  # once per epoch\n",
    "grad_clip = 1.0\n",
    "\n",
    "model.train()\n",
    "running_loss = 0.0\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(start_epoch + 1, EPOCHS + 1):\n",
    "    for step, (v1, v2) in enumerate(dl, start=1):\n",
    "        model.train()\n",
    "\n",
    "        # set scheduled LR\n",
    "        if total_steps > 0:\n",
    "            scale = lr_schedule(global_step)\n",
    "            for g in opt.param_groups:\n",
    "                g[\"lr\"] = LR * scale\n",
    "\n",
    "        v1, v2 = v1.to(device, non_blocking=True), v2.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            z1, z2 = model(v1), model(v2)\n",
    "            loss = info_nce_loss(z1, z2, temperature=0.2)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        # optional gradient clipping\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % log_every == 0:\n",
    "            avg = running_loss / log_every\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"[epoch {epoch:03d} | step {global_step:07d}] loss={avg:.4f} lr={opt.param_groups[0]['lr']:.2e} ({elapsed:.1f}s)\")\n",
    "            running_loss = 0.0\n",
    "            t0 = time.time()\n",
    "\n",
    "        # save latest checkpoint periodically (end of each epoch here)\n",
    "        if global_step % save_every_steps == 0:\n",
    "            save_ckpt(LATEST, epoch, global_step, best_running_loss, best_epoch)\n",
    "\n",
    "    # end-of-epoch evaluation proxy: use epoch average loss\n",
    "    # (running_loss resets mid-epoch; compute a quick average by re-looping is costly.\n",
    "    #  Here we use the last logged 'avg' or compute a lightweight moving average.)\n",
    "    # For a simple \"best\" gate, capture the last printed avg. If none printed in epoch, fall back to loss.item().\n",
    "    epoch_loss_estimate = float(loss.item())\n",
    "\n",
    "    if epoch_loss_estimate < best_running_loss:\n",
    "        best_running_loss = epoch_loss_estimate\n",
    "        best_epoch = epoch\n",
    "        save_ckpt(BEST, epoch, global_step, best_running_loss, best_epoch)\n",
    "        print(f\"Saved BEST at epoch {epoch} with proxy loss ~{best_running_loss:.4f}\")\n",
    "\n",
    "    # always refresh LATEST at end of epoch\n",
    "    save_ckpt(LATEST, epoch, global_step, best_running_loss, best_epoch)\n",
    "\n",
    "# --- export backbone-only checkpoint for downstream tasks (same filename pattern as before) ---\n",
    "final_ssl = (WORK_DIR/\"clips_metadata.csv\").with_suffix(\".ssl_r3d18.pt\")\n",
    "torch.save({\n",
    "    \"backbone_state_dict\": model.backbone.state_dict(),\n",
    "    \"projector_state_dict\": model.projector.state_dict(),\n",
    "    \"hparams\": dict(clip_frames=CLIP_FRAMES, target_fps=TARGET_FPS, short_side=SHORT_SIDE)\n",
    "}, final_ssl)\n",
    "print(f\"Exported backbone to: {final_ssl}\")\n",
    "\n",
    "# write compact metrics summary\n",
    "with open(METRICS_JSON, \"w\") as f:\n",
    "    json.dump({\"best_loss_proxy\": best_running_loss, \"best_epoch\": best_epoch,\n",
    "               \"final_global_step\": global_step}, f, indent=2)\n",
    "print(f\"Checkpoints in: {CKPT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cent",
   "language": "python",
   "name": "torch_cent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
